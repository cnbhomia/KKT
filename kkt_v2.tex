\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amssymb,amsmath,latexsym}
\usepackage{listings,microtype}
\usepackage{geometry}
\geometry{letterpaper, margin=1in}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  %postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}
\usepackage{comment}
\title{Developing accurate KKT formulation for NLP systems}

\begin{document}

\section{Introduction}

The article is aimed at assisting users interested in writing KKT conditions of their formulation and convert the optimization problem into
complementarity problem. Though GAMS allows users to solve non-linear problem (NLP) as an Mixed complementarity Problem (MCP) using the
EMP solver, it is often advantageous for users to explicitly model the KKT conditions and the respective marginals in their model.
This article focuses on providing a structured approach towards the same by systematically introducing the KKT
conditions and the marginals with their bounds. The article can be extended to help debug an existing KKT system in model.


Consider the simple example described in the \href{https://www.gams.com/latest/docs/S_PATH.html}{PATH} solver manual using the example of a Linear Transportation model.
The traditional model of fixed demand and price is made more realistic by making the variables endogenous, i.e. the price of commodity is market affects the demand of the
commodity, and the model is solved as a complementarily problem.  At times the complexity of the model leads to errors in formulation of optimality (KKT) conditions,
leading to incorrect or infeasible solutions. Finding the source of the error can be a particularly tedious task. The systematic scheme described in the article can
help users formulate error free MCP models for their NLP systems.

For a standard MCP tasked with finding a solution vector  $z \in \!R$ that is complementary to a function $F(z) : {\!R}^n \mapsto {\!R}^n$,
lower bounds $ l \in { \!R \cup {-\infty}}^n$ and upper bounds $ u \in { \!R \cup {\infty}}^n$ for each $ i \in {1,...,n}$, if one of the following three
condition holds then function $F$ is said to complementary to the variable $z$ and its bounds:

 \centerline{$F_{i}(z) = 0$  and  $ l_i \leq z_i \leq u_i $   or}
 \centerline{$F_{i}(z) > 0$  and  $ z_i = l_i$  or }
 \centerline{ $F_{i}(z) < 0$  and  $ z_i = u_i$ }

\noindent This is written in compact form as $ F(z) \perp L \leq z \leq U $, where the symbol $\perp$ means "perpendicular to". For a NLP model,
the optimization constraints and their respective marginals must be perpendicular to each other for an optimum solution to exist. Consider the
generalized minimization problem

\begin{equation}
\begin{aligned}
&	\min
& & f(x) \\
& \text{s.t.} & & 	 g_{i}(x) \leqslant 0	&	i = 1,2...m \\
& & &			h_{k}(x) = 0	 &	k = 1,2...p \\
& & &			d_{l}(x) \geqslant =0		&	l = 1,2...q \\
& & &			L \leq x \leq U \\
& & &			f(x): {\!R}^n \mapsto \!R , g(x): {\!R}^n \mapsto {\!R}^m\\
& & &			h(x): {\!R}^n \mapsto {\!R}^ p , d(x): {\!R}^n \mapsto {\!R}^ p\\
\end{aligned}
\end{equation}

\noindent For the above problem, the Lagrange function, KKT conditions and the multipliers in complementarity form can be written as \\

\begin{equation}
\begin{aligned}
& L(x,u,v,w) = f(x) - <u,g(x)> - <v,h(x)> - <w,d(x)>  \\
& \bigtriangledown_x L  \perp L_x \leq x \leq U_x 	\\
& - \bigtriangledown_u L  \perp u \geq 0	\\
& -\bigtriangledown_v L  \perp v free	\\
& -\bigtriangledown_w L  \perp w \leq 0	\\
\end{aligned}
\end{equation}

\noindent It should be noted that the gradient of Lagrangian w.r.t to the marginals from NLP is the constraint itself. Thus an NLP can be solved as an MCP
using the KKT conditions. However, formulating the KKT conditions might prove difficult for complex systems and may require verification of their accuracy.
In the following sections, we provide the framework for modeling the KKT conditions by using concept of dummy complementarity equations in the KKT formulation.
The process includes the following steps:


%--------------------------------------%--------------------------------------%--------------------------------------%--------------------------------------
\begin{comment}

\begin{equation}
\begin{aligned}
& \bigtriangledown{f(\hat{x})} - \sum_{i=1}^{m} u_{i} \bigtriangledown{g_{i}(\hat{x})}
			- \sum_{k=1}^{p} v_{i} \bigtriangledown{h_{k}(\hat{x})} - \sum_{l=1}^{q} w_{i} \bigtriangledown{d_{l}(\hat{x})} = 0  \\
\\
& h_{k}(\hat{x}) = 0   k = 1,2...p  \\
g_{i}(\hat{x}) \leq 0&	 i = 1,2...m \\  d_{l}(\hat{x}) \geq =0	&	l = 1,2...q
\\
and,\\
<u_{i},g_{i}(x)> = 0 \\ <v_{i},h_{k}(x)> =0 \\  <w_{l},d_{l}(x)> =0
\end{aligned}
\end{equation}

where $<u_{i},g_{i}(x)> = 0$  represent the complementarity condition and variables u, v, and w represent the marginals of the respective constraint. It is often written as

 $g_{i}(x) \perp L \leq u \leq U $

where symbol $\perp $(referred to as perpendicular  to) indicates pair-wise complementarity between the function g() and variable u and its bounds. The complementarity condition essentially

\end{comment}
%--------------------------------------%--------------------------------------%--------------------------------------%--------------------------------------


\begin{enumerate}
	\item Solve NLP model formulation and save the solution point using \href{https://www.gams.com/latest/docs/UG_SaveRestart.html}{Save and Restart} Feature
	\item	Reformulate the model as an MCP by adding dummy KKT conditions into the model.
  \item Solve the MCP using solution from step 1 as initial point. The MCP should start at the solution itself
	\item Replace one dummy equation at a time with one KKT condition. Resolve the NLP and save the results
	\item Restart the problem as MCP with iterlim=0. Objective value should match the results from NLP.
\end{enumerate}
\noindent Follow steps 4 and 5 until all KKT conditions have been successfully incorporated.


\section{Example : Maximum Revenue- NLP formulation}

Consider a simple case of a steel factory trying to maximize the revenue under budget constraints, with man-hours(h) and
raw materials steel (s) as the decision variables. The revenue is a function of decision variables given by

\centerline{$R(h,s) = 200 h^{(2/3)}s^{(1/3)} $ }
\bigbreak
\noindent where, budget = \$ 20,000, cost of manpower = \$ 20 /hr , cost of raw material = \$ 170 / tonn
The objective is to maximize revenue under the budget and manpower constraints.

\noindent As a standard minimization model, the problem can be formulated as:
\begin{equation}
\begin{aligned}
&	\min
& & R(h,s) = - 200 h^{2/3}s^{1/3}  \\
& \text{s.t.} & & 	 20h + 170s \leq 20000 \\
& & &			L< h,s < U   \\
\end{aligned}
\end{equation}

The GAMS program below solves for the following optimum values  \lstinputlisting{codes/factors.txt}
The results of the program are saved using command line option \textit{"save filename"}
\lstinputlisting {codes/sd_nlp.gms}

\noindent For the given system, the KKT conditions are :

\begin{equation}
\begin{aligned}
 L = & - 200 h^{2/3}s^{1/3} - con1\_m [ 20h + 170 s - 20000]	\\
 \bigtriangledown _h L = & - 200* (2/3) h^{(-1/3)}  s^{(1/3)} - con1\\_m*(20)  	\\
 \bigtriangledown _s L:  & - 200 * (1 / 3) h^{(2/3)} *(1/3) *  s^{(-2/3)} - con1\\_m*(170)   \\
 \bigtriangledown _{con1\_m} L : &   20*h + 170 * s - 20000 =0 \\
\end{aligned}
\end{equation}

\noindent It should be noted that the third KKT equation which is gradient of Lagrangean w.r.t to the marginal of constraint $con1$, is the budget
constraint from original NLP model. The above equations are solved as an MCP model using the results saved in the initial run using command line
option \textit{'restart filename'}.
In the model below, an error has been made in one of the KKT conditions. Execution of the model terminates due to the abort statement,
with declaration \textit{'We did not start at a solution'} as shown in the subsequent log.

\lstinputlisting{codes/sd_kkt.gms}
\lstinputlisting{codes/sd_kkt.log}

\noindent In case \textit{abort} statement is not used, the MCP formulationl will return a solution different from the NLP formulation, indicating
error in one of the KKT conditions. From simple observation, we can see that a typographical error was made with the coefficient of \textit{con1\_m}
in equation \textit{dLdh}, which when corrected provides solution consistent with the NLP formulation.

However, mere observation does not help with larger, more complex  systems. A strategy, as described in Section 1 is required to effectively correct the model.
In the given case, the MCP formulation can be first solved using dummy KKT conditions shown below. These conditions are then replaced
one at a time with the real KKT equations.

\lstinputlisting{codes/sd_kkt_dummy.gms}

\subsection{Rules for writing dummy KKT conditions}

Writing dummy condition, as straightforward as it may seem requires consideration of few salient points to avoid errors from the GAMS compiler.

\begin{enumerate}
	\item A new variable is needed to represent marginals of each constraint from NLP
  \item Number of dummy equations must equal number variables in the NLP formulation
	\item Dummy KKT condition must be accompanied by its differentiating variable fixed at the var.L value from the NLP solution
	\item KKT condition are modeled with the respective differentiating variable i.e. $dLdh \perp h$,
  \item NLP constratins are modeled with the respective marginals i.e. $con1 \perp con1\_m$
	\item All variables from both NLP and MCP formulation must appear in atleast one of the dummy KKT conditions
  \item Replacing a dummy equation must be accompanied by un-fixing the value of variable (var.L from step 3), per the concept of complementarity.

\end{enumerate}

For simplicity and to ensure that all variables are part of the model at all times, the variables can be placed in a single dummy equation replaced at the very end.


\section{KKT Development for complex systems}

In this section, we implement the proposed methodology and inferences from previous example to solve a more complex NLP problem as MCP.
The NLP example has been designed as a minimization model, which helps maintaining the sign convention. The complexity applies the methodology
over range of equations and contains leads/lags in the formulation. The NLP Model and the output is given below.

\lstinputlisting{codes/EX2.gms}

\lstinputlisting{codes/EX2.txt}


As seen in the model, the problem consists of five constraints and one objective variable. We define multiplier variables per the convention for each equations as follows:
\renewcommand\labelitemi{\tiny$\bullet$}
\begin{itemize}
	\item $sssdef\_m(i)$ for equation $sssdef(i)$ , free variable initialized at $sssdef.m(i)$
	\item $tttdef\_m(j))$ for equation $tttdef(j)$    , free variable initialized at $tttdef.m(i)$
	\item $esum\_m$ for equation $esum$		, positive variable initialized at $esum.m$
	\item $ssum\_m$ for equation $ssum$		, positive variable  initialized at $Ssum.m$
	\item $allbnd\_m$ for equation $allbnd$	, negative variable initialized at $allbnd.m$
\end{itemize}

The Lagrangian ($L$)of the function is given as

\begin{equation}
	L (sss,ttt,x,sss\_m,ttt\_m,esum\_m,ssum\_m,allbnd\_m) = obj - \sum_{i} sssdef\_m(i) . sssdef(i) -  \sum_{j} tttdef\_m(j) . tttdef(j) - (esum\_m . esum) - (ssum\_m . ssum) - (allbnd\_m . allbnd)
\end{equation}

The gradients of Lagrangian with respect to the marginals are

\begin{equation}
\begin{aligned}
\\
& dLdttt(j) =  \sum_{i} sssdef\_m(i)A(i,j) -  tttdef\_m(j)  - allbnd\_m \\
\\
& dLdsss(i) = 2(sss(i) - s0(i)) - sssdef(i)  -  ssum\_m -  esum\_m  , \quad ssum\_m \forall i2 \in i \\
\\
& dLdx(j) = c(j) + 4tttdef\_m(j) + tttdef\_m(j-1) - esum\_m \exp^{(x(j)-1)} - allbnd\_m , \quad \exp^{(x(j)-1)}  \forall j2 \in j \\
\\
\end{aligned}
\end{equation}

The gradients of Lagrangian with respect to the variables are the constraints themselves.
The model with dummy KKT conditions is shown below. When executed with saved solution from NLP model, the model exists at iteration 0 with the message
"solution found' as shown by the log below.

\lstinputlisting{codes/EX2KKT0.gms}
\lstinputlisting{codes/EX2KKT0.log}

\subsection{Replacing Dummy Equations}

The dummy  KKT equations representing gradient w.r.t the marginals are replaced one at a time per the process described in previous section.
We first replace the equation $dLdttt(j)$ with the real KKT equation as shown in the model below (using restart feature). Also, the associated complementarity
variable $ttt(i)$ is no longer fixed, which is the key to switching dummy equation from inactive to active. When executed with saved solution from NLP model,
the model exists at iteration 0 with the message "solution found' , i.e. the equation defining KKT condition $dLttt(i)$ is correct.

It should be noted that the multiplier $sssdef\_m(i)$, defined in equation $dLdsss(i)$ is now removed as it is already part of the model
in the new $dLdttt(j)$ equation. However, keeping $sssdef\_m(i)$ in the dummy equation would not affect the solution in any way because the variable
$sss(j)$ for the equation $dLdsss(i)$ is still fixed, allowing the dummy equation to take any value in the LHS (attribute to =n= equality).

\lstinputlisting{codes/EX2KKT1.gms}
\lstinputlisting{codes/EX2KKT1.log}

The remaining dummy equations are removed per the process described above, to obtain the final MCP model shown below. The model is initialized
by the results obtained from solving it as an NLP.

\lstinputlisting{codes/EX2KKT3.gms}
\lstinputlisting{codes/EX2KKT3.log}

\end{document}
