\documentclass{article}
\usepackage{graphicx}
\usepackage{amssymb,amsmath,latexsym}
\usepackage{listings,microtype}
\usepackage{geometry}
\geometry{letterpaper, margin=1in}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  %postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}
\usepackage{comment}
\title{Developing accurate KKT formulation for NLP systems}

\begin{document}

\section{Introduction}
Solving constrained non linear optimization problems begins with identification of the Karush-Kuhn-Tucker conditions. This is due to the fact that for a differentiable convex system where constraint qualification holds, the solution of NLP is the solution to the KKT system and vise versa. The KKT conditions enables solving the NLP model as an MCP model by representing the KKT conditions as complimentarily equations.  Thus, it is often advantageous to explicitly model the KKT conditions and solve them as a Mixed Complementarity Problem (MCP), as described in the PATH Solver Manual (/link) using the example of a Linear Transportation model. The traditional model of fixed demand and price is made more realistic by making the variables endogenous, i.e. the price of commodity is market affects the demand of the commodity, and the model is solved as a complimentarily problem.  At times the complexity of the model causes errors in formulation of KKT conditions, leading to incorrect or infeasible solutions. Finding the source of the error can be a particularly tedious task. Thus, in this article, we propose a systematic method for identification of errors in the explicit KKT conditions for solving an NLP. .

We begin by understanding the definition of complementarity. If a function $F(z) : {\!R}^n \mapsto {\!R}^n$, lower bounds $ l \in { \!R \cup {-\infty}}^n$ and upper bounds $ u \in { \!R \cup {\infty}}^n$ has a solution vector $z \in \!R$ such that for each $ i \in {1,...,n}$, one of the following three conditions hold :

 \centerline{$F_{i}(z) = 0$  and  $ l_i \leq z_i \leq u_i $   or}
 \centerline{$F_{i}(z) > 0$  and  $ z_i = l_i$  or }
 \centerline{ $F_{i}(z) < 0$  and  $ z_i = u_i$ }

then function $F$ is complementary to the variable $z$ and its bounds. This is written in compact form as

\centerline{ $ F(z) \perp L \leq z \leq U $ }
\par
\noindent Where the symbol $\perp$ means "perpendicular to".  From point of view of optimization, if $F(z)$ is non zero (non-binding constraint), changes in $z$ may further optimize the objective function until the constraint becomes binding. If $F(z)$ is binding, no changes in $z$ would further enhance the objective function, causing the marginal to be zero.

\noindent Consider the generalized NLP formulation below.

\begin{equation}
\begin{aligned}
&	\min
& & f(x) \\
& \text{s.t.} & & 	 g_{i}(x) \leqslant 0	&	i = 1,2...m \\
& & &			h_{k}(x) = 0	 &	k = 1,2...p \\
& & &			d_{l}(x) \geqslant =0		&	l = 1,2...q \\
& & &			L \leq x \leq Z \\
& & &			f(x): {\!R}^n \mapsto \!R , g(x): {\!R}^n \mapsto {\!R}^m\\
& & &			h(x): {\!R}^n \mapsto {\!R}^ p , d(x): {\!R}^n \mapsto {\!R}^ p\\
\end{aligned}
\end{equation}

\noindent The Lagrange function and KKT conditions for the formulation above in the complementarity form are written as \\

\begin{equation}
\begin{aligned}
& L(x,u,v,w) = f(x) - <u,g(x)> - <v,h(x)> - <w,d(x)>  \\
& \bigtriangledown_x L  \perp L_x \leq x \leq U_x 	\\
& - \bigtriangledown_u L  \perp u \geq 0	\\
& -\bigtriangledown_v L  \perp v free	\\
& -\bigtriangledown_w L  \perp w \leq 0	\\
\end{aligned}
\end{equation}

\noindent It should be noted that the gradient of Lagrangian w.r.t to the marginals from NLP result in the original inequality and equality constraints of the NLP.


%--------------------------------------%--------------------------------------%--------------------------------------%--------------------------------------
\begin{comment}

\begin{equation}
\begin{aligned}
& \bigtriangledown{f(\hat{x})} - \sum_{i=1}^{m} u_{i} \bigtriangledown{g_{i}(\hat{x})}
			- \sum_{k=1}^{p} v_{i} \bigtriangledown{h_{k}(\hat{x})} - \sum_{l=1}^{q} w_{i} \bigtriangledown{d_{l}(\hat{x})} = 0  \\
\\
& h_{k}(\hat{x}) = 0   k = 1,2...p  \\
g_{i}(\hat{x}) \leq 0&	 i = 1,2...m \\  d_{l}(\hat{x}) \geq =0	&	l = 1,2...q
\\
and,\\
<u_{i},g_{i}(x)> = 0 \\ <v_{i},h_{k}(x)> =0 \\  <w_{l},d_{l}(x)> =0
\end{aligned}
\end{equation}

where $<u_{i},g_{i}(x)> = 0$  represent the complimentarily condition and variables u, v, and w represent the marginals of the respective constraint. It is often written as

 $g_{i}(x) \perp L \leq u \leq U $

where symbol $\perp $(referred to as perpendicular  to) indicates pair-wise complementarity between the function g() and variable u and its bounds. The complimentairy condition essentially

\end{comment}
%--------------------------------------%--------------------------------------%--------------------------------------%--------------------------------------

\noindent Thus an NLP can be solved as an MCP using the KKT conditions. However, formulating the KKT conditions might prove difficult for complex systems, and require verification of their accuracy. In the following sections, we provide the framework for modeling the KKT conditions  by using concept of dummy complementarity equations in the KKT formulation. The process includes the following steps:

\begin{enumerate}
	\item Solve NLP formulation without explicit KKT conditions, and save the results
	\item	 Restart the problem as an MCP with a system of dummy KKT conditions using solution from step 1 as initial point. The MCP should start at the solution itself
	\item Replace one dummy equation at a time with one KKT condition. Resolve the NLP and save the results
	\item Restart the problem as MCP. If MCP iteration count is $>$ 0, there exists a problem with the KKT condition which was introduced.
\end{enumerate}
\noindent Follow steps 3 and 4 until all KKT conditions have been successfully incorporated.


\section{Example : Maximum Revenue- NLP formulation}

Consider a simple case of a steel factory trying to maximize the revenue under budget constraints, with man-hours(h) and raw materials steel (s) as the decision variables. The revenue is a function of decision variables given by

\centerline{$R(h,s) = 200 h^{(2/3)}s^{(1/3)} $ }
\bigbreak
\noindent where, budget = \$ 20,000
cost of manpower = \$ 20 /hr
cost of raw material = \$ 170 / tonn

\noindent In it's standard form, the model can be written as :

\begin{equation}
\begin{aligned}
&	\min
& & R(h,s) = - 200 h^{2/3}s^{1/3}  \\
& \text{s.t.} & & 	 20h + 170s \leq 20000 \\
& & &			L< h,s < U   \\
\end{aligned}
\end{equation}

The GAMS program below gives the optimum values of \lstinputlisting{codes/factors.txt}

\lstinputlisting {codes/sd_nlp.gms}

\noindent The above model is then written in form of an MCP, by explicitly adding the KKT conditions to the model. For the given system, the KKT conditions are given as

\begin{equation}
\begin{aligned}
 L = & - 200 h^{2/3}s^{1/3} - con1_m [ 20h + 170 s - 20000]	\\
 \bigtriangledown _h L = & - 200* (2/3) h^{(-1/3)}  s^{(1/3)} - con1\_m*(20)  	\\
 \bigtriangledown _s L:  & - 200 * (1 / 3) h^{(2/3)} *(1/3) *  s^{(-2/3)} - con1\_m*(170)   \\
 \bigtriangledown _{con1_m} L : &   20*h + 170 * s - 20000 =0 \\
\end{aligned}
\end{equation}

\noindent It should be noted that the third KKT equation, result of differentiation w.r.t to the marginal, is the inequality budget constraint from original NLP model. The above equations are solved as an MCP model as shown below using the results saved in the initial run. In the model below, an error has been made in one of the KKT conditions. Execution of the model terminates due to the abort statement, with declaration \textit{'we did not start at the solution'} as shown in the subsequent log. 

\lstinputlisting{codes/sd_kkt.gms}
\lstinputlisting{codes/sd_kkt.log}

\noindent Absence of \textit{abort} statement results in a new solution to the MCP where the results do not match the desired values from the original NLP formulation, also indicating error in one of the KKT conditions. From simple observation, we can see that an error was made in typing out the multiplier for variable \textit{con1\_m} in equation \textit{dLdh}, which when corrected provides results consistent with the NLP formulation. 

However, mere observation does not help with larger, more complex  systems. A strategy, as described in Section 1 is required to effectively correct the model. In the given case, the MCP formulation can be first solved using dummy KKT conditions described below. These conditions can then be replaced one at a time with the real KKT equations derived earlier. 

\lstinputlisting{codes/sd_kkt_dummy.gms}

\subsection{Rules for writing dummy KKT conditions}

Writing dummy condition, as straightforward as it may seem requires consideration of few salient points to avoid errors from the GAMS compiler.

\begin{enumerate}
	\item Number of dummy equations must equal number of marginal variables from the NLP formulation
	\item Each dummy KKT condition must be accompanied by its differentiating variable (of Lagrangian) 
	\item The value to differentiating variable is fixed at the marginal values provided by NLP solution
	\item Each KKT condition must be modeled as 'complimentary to' the differentiation variable for the condition. i.e. dLdh is $\perp$ to h, and is modeled as dLdh.h in the model statement
	\item Each variable representing the original constraint marginal (e.g con1\_m) in the MCP model must appear in one of the KKT conditions. 
	
\end{enumerate}

Thus, all variables representing the marginals of NLP constraints can be incorporated in any one of the dummy equation. This equation should be replaced by KKT condition at the very end. However, in the current case, variable 'con1\_m' , representing marginal of constraint 'con1' from NLP formulation is part of both the KKT conditions. Thus, though incorporated in equation dLds , the order of replacing the dummy equation with KKT condition is irrelevant in the given case, but can be better understood in the example described in subsequent section

\end{document}
