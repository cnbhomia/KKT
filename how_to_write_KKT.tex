\documentclass{article}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amssymb,amsmath,latexsym}
\usepackage{listings,microtype}
\usepackage{geometry}
\geometry{letterpaper, margin=1in}
\lstset{
  basicstyle=\ttfamily,
  columns=fullflexible,
  frame=single,
  breaklines=true,
  %postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}
\usepackage{comment}
\title{Developing accurate KKT formulation for NLP systems}

\begin{document}

\section{Introduction}

The article is aimed at assisting users interested in writing KKT conditions of their formulation and convert the optimization problem into
complimentarity problem. Though GAMS allows users to solve non-linear problem (NLP) as an Mixed Complimentarity Problem (MCP) using the
EMP solver, it is often advantageous for users to explicitly model the KKT conditions and the respective marginals in their model.
This article focuses on providing a structured approach towards reformulating NLP model as MCP model by systematically introducing the KKT
conditions, with marginals and their bounds. The article can be extended to help debug an existing KKT system in model.


Consider the simple example described in the \href{https://www.gams.com/latest/docs/S_PATH.html}{PATH} solver manual using the example of a Linear Transportation model.
The traditional model of fixed demand and price is made more realistic by making the variables endogenous, i.e. the price of commodity affects it's demand, and the model
is solved as a complimentarily problem.  At times, formulating optimality conditions is not as straightforward due to the complexity of the model, causing errors
leading to incorrect or infeasible solutions. Finding the source of the error can be a particularly tedious task. The systematic scheme described in the article can
help users formulate error free MCP models for their NLP systems.

For a standard MCP tasked with finding a solution vector  $z \in \!R$ that is complimentary to a function $F(z) : {\!R}^n \mapsto {\!R}^n$,
lower bounds $ l \in { \!R \cup {-\infty}}^n$ and upper bounds $ u \in { \!R \cup {\infty}}^n$ for each $ i \in {1,...,n}$, if one of the following three
condition holds then function $F$ is said to complementary to the variable $z$ and its bounds:

 \centerline{$F_{i}(z) = 0$  and  $ l_i \leq z_i \leq u_i $   or}
 \centerline{$F_{i}(z) > 0$  and  $ z_i = l_i$  or }
 \centerline{ $F_{i}(z) < 0$  and  $ z_i = u_i$ }

\noindent This is written in compact form as:

\centerline{ $ F(z) \perp L \leq z \leq U $ }
\par
\noindent where the symbol $\perp$ means "perpendicular to". For a NLP model, the optimization constraints and their respective marginals must be
perpendicular to each other for an optimum solution to exist. Consider the following generalized NLP formulation.

\begin{equation}
\begin{aligned}
&	\min
& & f(x) \\
& \text{s.t.} & & 	 g_{i}(x) \leqslant 0	&	i = 1,2...m \\
& & &			h_{k}(x) = 0	 &	k = 1,2...p \\
& & &			d_{l}(x) \geqslant =0		&	l = 1,2...q \\
& & &			L \leq x \leq U \\
& & &			f(x): {\!R}^n \mapsto \!R , g(x): {\!R}^n \mapsto {\!R}^m\\
& & &			h(x): {\!R}^n \mapsto {\!R}^ p , d(x): {\!R}^n \mapsto {\!R}^ p\\
\end{aligned}
\end{equation}

\noindent The Lagrange function, KKT conditions and their multipliers in complementarity form can be written as \\

\begin{equation}
\begin{aligned}
& L(x,u,v,w) = f(x) - <u,g(x)> - <v,h(x)> - <w,d(x)>  \\
& \bigtriangledown_x L  \perp L_x \leq x \leq U_x 	\\
& - \bigtriangledown_u L  \perp u \geq 0	\\
& -\bigtriangledown_v L  \perp v free	\\
& -\bigtriangledown_w L  \perp w \leq 0	\\
\end{aligned}
\end{equation}

\noindent It should be noted that the gradient of Lagrangian w.r.t to the marginals from NLP is the constraint itself. However, formulating the KKT conditions
might prove difficult for complex systems, and require verification of their accuracy. In the following sections, we provide the framework for modeling the
KKT conditions as complementarity conditions by using concept of dummy complementarity equations in the KKT formulation.
The process includes the following steps:

\begin{enumerate}
	\item Solve NLP model formulation, and save the results using \href{https://www.gams.com/latest/docs/UG_SaveRestart.html}{Save and Restart Feature}
	\item	Reformulate the model as an MCP by adding dummy KKT conditions into the model.
  \item Solve the MCP using solution from step 1 as initial point. The MCP should start at the solution itself
	\item Replace one dummy equation at a time with one KKT condition.
	\item Restart the problem as MCP with iterlim=0. Objective value should match the results from NLP.
\end{enumerate}
\noindent Iterate through steps 3-5 until all KKT conditions have been successfully incorporated.


\section{Example : Maximum Revenue- NLP formulation}

Consider a simple case of a steel factory trying to maximize the revenue under budget constraints, with man-hours(h) and
raw materials steel (s) as the decision variables. The revenue is a function of decision variables given by

\centerline{$R(h,s) = 200 h^{(2/3)}s^{(1/3)} $ }
\bigbreak
\noindent where, budget = \$ 20,000, cost of manpower = \$ 20 /hr , cost of raw material = \$ 170 / tonn
The objective is to maximize revenue under the budget and manpower constraints.

\noindent As a standard minimization model, the problem can be formulated as:
\begin{equation}
\begin{aligned}
&	\min
& & R(h,s) = - 200 h^{2/3}s^{1/3}  \\
& \text{s.t.} & & 	 20h + 170s \leq 20000 \\
& & &			L< (h,s) < U   \\
\end{aligned}
\end{equation}

The GAMS program below gives the optimum values of \lstinputlisting{codes/factors.txt}
The results of the program are saved using command line option \textit{"save filename"}
\lstinputlisting {codes/sd_nlp.gms}

\noindent For the above problem, the KKT conditions are given by

\begin{equation}
\begin{aligned}
 L = & - 200 h^{2/3}s^{1/3} - con1\_m [ 20h + 170 s - 20000]	\\
 \bigtriangledown_h L = & - 200* (2/3) h^{(-1/3)}  s^{(1/3)} - con1\_m*(20)  	\\
 \bigtriangledown_s L:  & - 200 * (1 / 3) h^{(2/3)} *(1/3) *  s^{(-2/3)} - con1\_m*(170)   \\
 \bigtriangledown_{con1\_m} L : &   20*h + 170 * s - 20000 =0 \\
\end{aligned}
\end{equation}
where $con1\_m$ is the marginal of constraint 'con1' in the original NLP formulation.

\noindent It should be noted that the third KKT equation which is gradient of Lagrangean w.r.t to the marginal, is the inequality budget
constraint from original NLP model. The above equations are solved as an MCP model using the results saved in the initial run using command line
option \textit{'restart filename'}.
In presence of an error in the KKT formulation, the iteration limit causes model termination during execution.
In the GAMS model below, an error has been made in writing the KKT conditions, resulting in termination
with declaration \textit{'We did not start at a solution'}, as shown in the subsequent log.

\lstinputlisting{codes/sd_kkt.gms}
\lstinputlisting{codes/sd_kkt.log}

\noindent In case \textit{abort} statement is not used, the MCP formulationl will return a solution different from the NLP formulation, indicating
error in one of the KKT conditions. From simple observation, we can see that a typographical error was made with the multiplier of \textit{con1\_m}
in equation \textit{dLdh}, which when corrected provides results consistent with the NLP formulation.

However, mere observation does not help with larger, more complex  systems. A strategy, as described in Section 1 is required to effectively correct the model.
In the given case, the MCP formulation can be first solved using dummy KKT conditions shown below. These conditions can then be replaced
one at a time with the real KKT equations derived earlier.

\lstinputlisting{codes/sd_kkt_dummy.gms}

\subsection{Rules for writing dummy KKT conditions}

Writing dummy condition, as straightforward as it may seem requires consideration of few salient points to avoid errors from the GAMS compiler.

\begin{enumerate}
	\item A new variable is needed to represent marginals of each constraint from NLP (eg. $con1\_m$ from above example)
  \item Number of dummy equations must equal number of variables in the NLP formulation
	\item Each dummy KKT condition must be accompanied by its differentiating variable fixed at the var.L value from the saved run of NLP
	\item Each KKT condition must be modeled with the differentiating variable for the condition. i.e. $dLdh \perp h$,
        and is modeled as $dLdh.h$ in the model statement
	\item All variables representing NLP constraint marginals must in atleast one of the dummy KKT conditions
  \item Replacing a dummy equation must be accompanied by un-fixing the value of variable, per the concept of complimentarity.

\end{enumerate}

\section{KKT Development for Complex Formulations}

In this section, we implement the proposed methodology and inferences from previous example to solve a more complex NLP problem as MCP.
The NLP example has been designed as a minimization model, which helps maintaining the sign convention. The complexity applies the methodology
over range of equations and contains leads/lags in the formulation. The NLP Model and the output is given below.

\lstinputlisting{codes/EX2.gms}

\lstinputlisting{codes/EX2.txt}


As seen in the model, the problem consists of five constraints and one objective variable. We define multiplier variables per the convention for each equations as follows:
\renewcommand\labelitemi{\tiny$\bullet$}
\begin{itemize}
	\item $sssdef\_m(i)$ for equation $sssdef(i)$ , free variable initialized at $sssdef.m(i)$
	\item $tttdef\_m(j))$ for equation $tttdef(j)$    , free variable initialized at $tttdef.m(i)$
	\item $esum\_m$ for equation $esum$		, positive variable initialized at $esum.m$
	\item $ssum\_m$ for equation $ssum$		, positive variable  initialized at $Ssum.m$
	\item $allbnd\_m$ for equation $allbnd$	, negative variable initialized at $allbnd.m$
\end{itemize}

The Lagrangian ($L$)of the function is given as

\begin{equation}
  \begin{aligned}
	L (sss,ttt,x,sss\_m,ttt\_m,esum\_m,ssum\_m,allbnd\_m) = obj - \sum_{i} sssdef\_m(i) . sssdef(i) - \\
   \sum_{j} tttdef\_m(j) . tttdef(j) - (esum\_m . esum) - (ssum\_m . ssum) - (allbnd\_m . allbnd)
 \end{aligned}
\end{equation}

The gradients of Lagrangian with respect to the marginals are

\begin{equation}
\begin{aligned}
\\
& dLdttt(j) =  \sum_{i} sssdef\_m(i)A(i,j) -  tttdef\_m(j)  - allbnd\_m \\
\\
& dLdsss(i) = 2(sss(i) - s0(i)) - sssdef(i)  -  ssum\_m -  esum\_m  , \quad ssum\_m \forall i2 \in i \\
\\
& dLdx(j) = c(j) + 4tttdef\_m(j) + tttdef\_m(j-1) - esum\_m \exp^{(x(j)-1)} - allbnd\_m , \quad \exp^{(x(j)-1)}  \forall j2 \in j \\
\\
\end{aligned}
\end{equation}

The gradients of Lagrangian with respect to the variables are the constraints themselves.
The model with dummy KKT conditions is shown below. When executed with saved solution from NLP model, the model exists at iteration 0 with the message
"solution found' as shown by the log below.

\lstinputlisting{codes/EX2KKT0.gms}
\lstinputlisting{codes/EX2KKT0.log}

\subsection{Replacing Dummy Equations}

The dummy  KKT equations representing gradient w.r.t the marginals are replaced one at a time per the process described in previous section.
We first replace the equation $dLdttt(j)$ with the real KKT equation as shown in the model below (using restart feature). Also, the associated complementarity
variable $ttt(i)$ is no longer fixed, which is the key to switching dummy equation from inactive to active. When executed with saved solution from NLP model,
the model exists at iteration 0 with the message "solution found' , i.e. the equation defining KKT condition $dLttt(i)$ is correct.


It should be noted that the multiplier $sssdef\_m(i)$, defined in equation $dLdsss(i)$ is now removed as it is already part of the model
in the new $dLdttt(j)$ equation. However, keeping $sssdef\_m(i)$ in the dummy equation would not affect the solution in any way because the variable
$sss(j)$ for the equation $dLdsss(i)$ is still fixed, allowing the dummy equation to take any value in the LHS (attribute to =n= equality).

\lstinputlisting{codes/EX2KKT1.gms}
\lstinputlisting{codes/EX2KKT1.log}

The remaining dummy equations are removed per the process described above, to obtain the final MCP model shown below. The model is initialized
by the results obtained from solving it as an NLP.

\lstinputlisting{codes/EX2KKT3.gms}
\lstinputlisting{codes/EX2KKT3.log}

\end{document}
